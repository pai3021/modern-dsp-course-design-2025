"""
è®­ç»ƒä¸éªŒè¯æ¨¡å— - ä»…ç”¨äºæ¨¡å‹å¼€å‘å’Œäº¤å‰éªŒè¯
"""
import os
import numpy as np
import pickle
from datetime import datetime
from algorithm_f1_optimized import BalancedClassifier as RuleBasedClassifier
import config
from features import extract_features_with_cache

def evaluate_metrics(y_true, y_pred):
    """è®¡ç®—å‡†ç¡®ç‡ã€F1ç­‰æŒ‡æ ‡"""
    TP = sum((y_true == 'X') & (y_pred == 'X'))
    TN = sum((y_true == 'N') & (y_pred == 'N'))
    FP = sum((y_true == 'N') & (y_pred == 'X'))
    FN = sum((y_true == 'X') & (y_pred == 'N'))
    
    acc = (TP + TN) / len(y_true) if len(y_true) > 0 else 0
    precision = TP / (TP + FP + 1e-6)
    recall = TP / (TP + FN + 1e-6)
    f1 = 2 * (precision * recall) / (precision + recall + 1e-6)
    
    print(f"  [ç»“æœ] Acc: {acc:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}")
    print(f"  [è¯¦æƒ…] TP:{TP}  TN:{TN}  FP:{FP}  FN:{FN}")
    print(f"  [åˆ†æ] å¼‚å¸¸æ€»æ•°:{TP+FN}  è¯¯æŠ¥ç‡:{FP/(TN+FP+1e-6):.4f}")
    
    return {'acc': acc, 'precision': precision, 'recall': recall, 'f1': f1,
            'TP': TP, 'TN': TN, 'FP': FP, 'FN': FN}

def process_files(file_list, is_training=True, use_full_data=False, use_cache=True):
    """è¯»å–æ–‡ä»¶åˆ—è¡¨å¹¶æå–ç‰¹å¾ï¼ˆæ”¯æŒç¼“å­˜åŠ é€Ÿï¼‰"""
    all_features = []
    all_labels = []
    all_file_info = []  # è®°å½•æ¯ä¸ªå¿ƒæ‹æ¥è‡ªå“ªä¸ªæ–‡ä»¶
    
    cache_status = "ç¼“å­˜:å¼€å¯ ğŸš€" if use_cache else "ç¼“å­˜:å…³é—­"
    print(f"\næ­£åœ¨å¤„ç† {'è®­ç»ƒ' if is_training else 'æµ‹è¯•'} æ•°æ®é›†... ({cache_status})")
    
    for fname in file_list:
        print(f"-> {fname}", end=" ")
        
        try:
            # ä½¿ç”¨ç¼“å­˜åŠ è½½ç‰¹å¾
            X_file, y_file, r_indices, valid_beat_indices = extract_features_with_cache(
                fname, use_cache=use_cache
            )
            
            # è°ƒè¯•æ¨¡å¼é™åˆ¶æ•°æ®é‡
            if not use_full_data and len(X_file) > 5000:
                print(f"[è°ƒè¯•:å‰5000/{len(X_file)}]", end=" ")
                X_file = X_file[:5000]
                y_file = y_file[:5000]
            
            all_features.append(X_file)
            
            if is_training:
                all_labels.append(y_file)
                all_file_info.extend([fname] * len(X_file))
                
                # ç»Ÿè®¡
                n_abnormal = sum(y_file == 'X')
                abnormal_rate = n_abnormal / len(y_file) * 100
                print(f"âœ“ {len(X_file)}å¿ƒæ‹, å¼‚å¸¸:{n_abnormal}({abnormal_rate:.1f}%)")
            else:
                print(f"âœ“ {len(X_file)}å¿ƒæ‹")
                
        except Exception as e:
            print(f"âœ— é”™è¯¯: {e}")
            continue
    
    X = np.vstack(all_features)
    
    if is_training:
        y = np.concatenate(all_labels)
        total_abnormal = sum(y == 'X')
        print(f"\n[æ±‡æ€»] ç‰¹å¾:{X.shape}, å¼‚å¸¸:{total_abnormal}/{len(y)}({total_abnormal/len(y)*100:.2f}%)")
        return X, y, all_file_info
    else:
        return X

def analyze_failed_cases(X, y, y_pred, file_info, fold_name):
    """åˆ†æé¢„æµ‹å¤±è´¥çš„æ¡ˆä¾‹"""
    print(f"\n[æ·±åº¦åˆ†æ] {fold_name}")
    
    # æ‰¾å‡ºå‡é˜´æ€§ï¼ˆæ¼æ£€ï¼‰
    fn_indices = np.where((y == 'X') & (y_pred == 'N'))[0]
    if len(fn_indices) > 0:
        print(f"  å‡é˜´æ€§ï¼ˆæ¼æ£€ï¼‰æ ·æœ¬æ•°: {len(fn_indices)}")
        # åˆ†æè¿™äº›æ ·æœ¬çš„ç‰¹å¾åˆ†å¸ƒ
        fn_features = X[fn_indices]
        print(f"  æ¼æ£€æ ·æœ¬RRæ¯”ç‡å‡å€¼: {np.mean(fn_features[:, 0]):.3f}")
        print(f"  æ¼æ£€æ ·æœ¬RRæ¯”ç‡æœ€å°: {np.min(fn_features[:, 0]):.3f}")
        
    # æ‰¾å‡ºå‡é˜³æ€§ï¼ˆè¯¯æŠ¥ï¼‰
    fp_indices = np.where((y == 'N') & (y_pred == 'X'))[0]
    if len(fp_indices) > 0:
        print(f"  å‡é˜³æ€§ï¼ˆè¯¯æŠ¥ï¼‰æ ·æœ¬æ•°: {len(fp_indices)}")
        fp_features = X[fp_indices]
        print(f"  è¯¯æŠ¥æ ·æœ¬RRæ¯”ç‡å‡å€¼: {np.mean(fp_features[:, 0]):.3f}")

def run_loocv(use_full_data=False, use_cache=True):
    """ç•™ä¸€äº¤å‰éªŒè¯ï¼ˆæ”¯æŒç¼“å­˜åŠ é€Ÿï¼‰"""
    print("\n" + "="*70)
    print("=== ç•™ä¸€äº¤å‰éªŒè¯ï¼ˆLOOCVï¼‰===")
    print("="*70)
    
    all_files = config.TRAIN_FILES
    results = []
    
    import time
    total_start = time.time()
    
    for i, test_file in enumerate(all_files):
        print(f"\n{'='*70}")
        print(f"Fold {i+1}/{len(all_files)}: æµ‹è¯• {test_file}")
        print(f"{'='*70}")
        
        fold_start = time.time()
        
        # ç•™ä¸€æ³•åˆ’åˆ†
        train_files = [f for f in all_files if f != test_file]
        
        # è®­ç»ƒ
        print("\n[è®­ç»ƒé˜¶æ®µ]")
        X_train, y_train, _ = process_files(train_files, is_training=True, 
                                           use_full_data=use_full_data, use_cache=use_cache)
        
        model = RuleBasedClassifier()
        model.fit(X_train, y_train)
        
        # æµ‹è¯•
        print("\n[æµ‹è¯•é˜¶æ®µ]")
        X_test, y_test, file_info = process_files([test_file], is_training=True, 
                                                  use_full_data=use_full_data, use_cache=use_cache)
        
        y_pred = model.predict(X_test)
        metrics = evaluate_metrics(y_test, y_pred)
        
        # æ·±åº¦åˆ†æ
        analyze_failed_cases(X_test, y_test, y_pred, file_info, test_file)
        
        # ä¿å­˜ç»“æœ
        metrics['file'] = test_file
        results.append(metrics)
        
        fold_time = time.time() - fold_start
        print(f"\nâ±ï¸  æœ¬æŠ˜ç”¨æ—¶: {fold_time:.1f}ç§’")
    
    # æ±‡æ€»æŠ¥å‘Š
    print("\n" + "="*60)
    print("=== äº¤å‰éªŒè¯æ±‡æ€» ===")
    print("="*60)
    
    print(f"\n{'æ–‡ä»¶å':<20} {'Acc':<8} {'Recall':<8} {'F1':<8} {'å¼‚å¸¸æ•°':<8}")
    print("-"*60)
    for r in results:
        print(f"{r['file']:<20} {r['acc']:<8.4f} {r['recall']:<8.4f} {r['f1']:<8.4f} {r['TP']+r['FN']:<8}")
    
    print("\n" + "-"*60)
    accs = [r['acc'] for r in results]
    recalls = [r['recall'] for r in results]
    f1s = [r['f1'] for r in results]
    
    print(f"å¹³å‡ Accuracy:  {np.mean(accs):.4f} Â± {np.std(accs):.4f}")
    print(f"å¹³å‡ Recall:    {np.mean(recalls):.4f} Â± {np.std(recalls):.4f}")
    print(f"å¹³å‡ F1:        {np.mean(f1s):.4f} Â± {np.std(f1s):.4f}")
    
    total_time = time.time() - total_start
    print(f"\nâ±ï¸  LOOCVæ€»ç”¨æ—¶: {total_time/60:.1f}åˆ†é’Ÿ")
    
    return results

def train_final_model(use_full_data=False, use_cache=True, save_model=True):
    """ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®è®­ç»ƒæœ€ç»ˆæ¨¡å‹"""
    print("\n" + "="*70)
    print("=== è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®ï¼‰===")
    print("="*70)
    
    X_train, y_train, _ = process_files(config.TRAIN_FILES, is_training=True, 
                                       use_full_data=use_full_data, use_cache=use_cache)
    
    model = RuleBasedClassifier()
    model.fit(X_train, y_train)
    
    # ä¿å­˜æ¨¡å‹
    if save_model:
        model_dir = os.path.join(os.path.dirname(__file__), 'models')
        os.makedirs(model_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_path = os.path.join(model_dir, f"model_{timestamp}.pkl")
        
        with open(model_path, 'wb') as f:
            pickle.dump(model, f)
        
        print(f"\n[æ¨¡å‹ä¿å­˜] {model_path}")
        
        # åŒæ—¶ä¿å­˜ä¸€ä¸ª"latest"ç‰ˆæœ¬æ–¹ä¾¿é¢„æµ‹æ—¶åŠ è½½
        latest_path = os.path.join(model_dir, "model_latest.pkl")
        with open(latest_path, 'wb') as f:
            pickle.dump(model, f)
        print(f"[æ¨¡å‹ä¿å­˜] {latest_path}")
    
    return model

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='è®­ç»ƒä¸éªŒè¯å¿ƒæ‹åˆ†ç±»æ¨¡å‹')
    parser.add_argument('--full', action='store_true', help='ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼ˆé»˜è®¤åªç”¨å‰5000æ¡è°ƒè¯•ï¼‰')
    parser.add_argument('--skip-cv', action='store_true', help='è·³è¿‡äº¤å‰éªŒè¯ï¼Œç›´æ¥è®­ç»ƒæœ€ç»ˆæ¨¡å‹')
    parser.add_argument('--no-cache', action='store_true', help='ä¸ä½¿ç”¨ç‰¹å¾ç¼“å­˜ï¼ˆè°ƒè¯•ç”¨ï¼‰')
    
    args = parser.parse_args()
    
    use_cache = not args.no_cache
    
    if args.full:
        print("\nâš ï¸  ä½¿ç”¨å…¨éƒ¨æ•°æ®æ¨¡å¼ï¼Œè®­ç»ƒæ—¶é—´ä¼šè¾ƒé•¿...")
    else:
        print("\nğŸ’¡ è°ƒè¯•æ¨¡å¼ï¼šæ¯ä¸ªæ–‡ä»¶åªç”¨å‰5000ä¸ªå¿ƒæ‹")
        print("   æ­£å¼è®­ç»ƒè¯·åŠ  --full å‚æ•°")
    
    if use_cache:
        print("ğŸš€ ç‰¹å¾ç¼“å­˜å·²å¯ç”¨ï¼ˆé¦–æ¬¡éœ€è¿è¡Œ python preprocess_features.pyï¼‰")
    else:
        print("âš ï¸  ç‰¹å¾ç¼“å­˜å·²ç¦ç”¨ï¼Œå°†é‡æ–°æå–ç‰¹å¾")
    print()
    
    # ç•™ä¸€äº¤å‰éªŒè¯
    if not args.skip_cv:
        cv_results = run_loocv(use_full_data=args.full, use_cache=use_cache)
    
    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    final_model = train_final_model(use_full_data=args.full, use_cache=use_cache, save_model=True)
    
    print("\nâœ… è®­ç»ƒå®Œæˆï¼ç°åœ¨å¯ä»¥è¿è¡Œ predict.py è¿›è¡Œæµ‹è¯•é›†é¢„æµ‹")
